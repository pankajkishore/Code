{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# def mnist_data_iteratior():\n",
    "#     mnist = input_data.read_data_sets('./newdata', one_hot=True)\n",
    "#     def iterator(hparams, num_batches):\n",
    "#         for _ in range(num_batches):\n",
    "#             yield mnist.train.next_batch(hparams.batch_size)\n",
    "#     return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_data_iteratior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(hparams, x_ph, scope_name, reuse):\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        w1 = tf.get_variable('w1', initializer=xavier_init(hparams.n_input, hparams.n_hidden_recog_1))\n",
    "        b1 = tf.get_variable('b1', initializer=tf.zeros([hparams.n_hidden_recog_1], dtype=tf.float32))\n",
    "        hidden1 = hparams.transfer_fct(tf.matmul(x_ph, w1) + b1)\n",
    "\n",
    "        w2 = tf.get_variable('w2', initializer=xavier_init(hparams.n_hidden_recog_1, hparams.n_hidden_recog_2))\n",
    "        b2 = tf.get_variable('b2', initializer=tf.zeros([hparams.n_hidden_recog_2], dtype=tf.float32))\n",
    "        hidden2 = hparams.transfer_fct(tf.matmul(hidden1, w2) + b2)\n",
    "\n",
    "        w3 = tf.get_variable('w3', initializer=xavier_init(hparams.n_hidden_recog_2, hparams.n_z))\n",
    "        b3 = tf.get_variable('b3', initializer=tf.zeros([hparams.n_z], dtype=tf.float32))\n",
    "        z_mean = tf.matmul(hidden2, w3) + b3\n",
    "\n",
    "        w4 = tf.get_variable('w4', initializer=xavier_init(hparams.n_hidden_recog_2, hparams.n_z))\n",
    "        b4 = tf.get_variable('b4', initializer=tf.zeros([hparams.n_z], dtype=tf.float32))\n",
    "        z_log_sigma_sq = tf.matmul(hidden2, w4) + b4\n",
    "\n",
    "    return z_mean, z_log_sigma_sq\n",
    "\n",
    "\n",
    "def generator(hparams, z, scope_name, reuse):\n",
    "\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        w1 = tf.get_variable('w1', initializer=xavier_init(hparams.n_z, hparams.n_hidden_gener_1))\n",
    "        b1 = tf.get_variable('b1', initializer=tf.zeros([hparams.n_hidden_gener_1], dtype=tf.float32))\n",
    "        hidden1 = hparams.transfer_fct(tf.matmul(z, w1) + b1)\n",
    "\n",
    "        w2 = tf.get_variable('w2', initializer=xavier_init(hparams.n_hidden_gener_1, hparams.n_hidden_gener_2))\n",
    "        b2 = tf.get_variable('b2', initializer=tf.zeros([hparams.n_hidden_gener_2], dtype=tf.float32))\n",
    "        hidden2 = hparams.transfer_fct(tf.matmul(hidden1, w2) + b2)\n",
    "\n",
    "        w3 = tf.get_variable('w3', initializer=xavier_init(hparams.n_hidden_gener_2, hparams.n_input))\n",
    "        b3 = tf.get_variable('b3', initializer=tf.zeros([hparams.n_input], dtype=tf.float32))\n",
    "        logits = tf.matmul(hidden2, w3) + b3\n",
    "        x_reconstr_mean = tf.nn.sigmoid(logits)\n",
    "\n",
    "    return logits, x_reconstr_mean\n",
    "\n",
    "\n",
    "def get_loss(x, logits, z_mean, z_log_sigma_sq):\n",
    "    reconstr_losses = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=logits), 1)\n",
    "    latent_losses = -0.5 * tf.reduce_sum(1 + z_log_sigma_sq - tf.square(z_mean) - tf.exp(z_log_sigma_sq), 1)\n",
    "    total_loss = tf.reduce_mean(reconstr_losses + latent_losses, name='total_loss')\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def get_z_var(hparams, batch_size):\n",
    "    z = tf.Variable(tf.random_normal((batch_size, hparams.n_z)), name='z')\n",
    "    return z\n",
    "\n",
    "\n",
    "def gen_restore_vars():\n",
    "    restore_vars = ['gen/w1',\n",
    "                    'gen/b1',\n",
    "                    'gen/w2',\n",
    "                    'gen/b2',\n",
    "                    'gen/w3',\n",
    "                    'gen/b3']\n",
    "    return restore_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, size, image_path):\n",
    "    print('----------------------------------saveeeeeeeeeeeeee---------',images.shape)\n",
    "    print(size)\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2\n",
    "\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "    return img\n",
    "\n",
    "\n",
    "def print_hparams(hparams):\n",
    "    print ('')\n",
    "    for temp in dir(hparams):\n",
    "        if temp[:1] != '_':\n",
    "            print ('{0} = {1}'.format(temp, getattr(hparams, temp)))\n",
    "    print ('')\n",
    "\n",
    "\n",
    "def set_up_dir(directory, clean=False):\n",
    "    if os.path.exists(directory):\n",
    "        if clean:\n",
    "            shutil.rmtree(directory)\n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def get_ckpt_path(ckpt_dir):\n",
    "    ckpt_dir = os.path.abspath(ckpt_dir)\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_path = os.path.join(ckpt_dir,\n",
    "                                 ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        ckpt_path = None\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def try_restore(hparams, sess, model_saver):\n",
    "    \"\"\"Attempt to restore variables from checkpoint\"\"\"\n",
    "    ckpt_path = get_ckpt_path(hparams.ckpt_dir)\n",
    "    if ckpt_path:  # if a previous ckpt exists\n",
    "        model_saver.restore(sess, ckpt_path)\n",
    "        start_epoch = int(ckpt_path.split('/')[-1].split('-')[-1])\n",
    "        print ('Succesfully loaded model from {0} at counter = {1}'.format(\n",
    "            ckpt_path, start_epoch))\n",
    "    else:\n",
    "        print ('No checkpoint found')\n",
    "        start_epoch = -1\n",
    "    return start_epoch\n",
    "\n",
    "\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant*np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hparams(object):\n",
    "    def __init__(self):\n",
    "        self.n_hidden_recog_1 = 500  # 1st layer encoder neurons\n",
    "        self.n_hidden_recog_2 = 500  # 2nd layer encoder neurons\n",
    "        self.n_hidden_gener_1 = 500  # 1st layer decoder neurons\n",
    "        self.n_hidden_gener_2 = 500  # 2nd layer decoder neurons\n",
    "        self.n_input = 784           # MNIST data input (img shape: 28*28)\n",
    "        self.n_z = 20                # dimensionality of latent space\n",
    "        self.transfer_fct = tf.nn.softplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory_path):\n",
    "    print(len(os.listdir(directory_path)))\n",
    "    train_data = []\n",
    "    for file_ in sorted(os.listdir(directory_path)):\n",
    "        if file_.endswith('.png'):\n",
    "            if directory_path.endswith('/'):\n",
    "                 image_path = directory_path + file_\n",
    "            else: image_path = directory_path + '/' + file_\n",
    "\n",
    "            image = imread(image_path)/255.0 # Normalize values\n",
    "            image = np.resize(image, (28,28))# added pankaj\n",
    "            image = np.expand_dims(image, axis=-1) # Add channel dim\n",
    "            train_data.append(image)\n",
    "    train_data = np.array(train_data)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_data('resized/')[:3900]\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = []\n",
    "\n",
    "# for i in range(39):\n",
    "#     final_data.append(data[(0+i)*100:(1+i)*100])\n",
    "\n",
    "# final_data = np.array(final_data)\n",
    "\n",
    "# final_data[0].shape\n",
    "\n",
    "# final_list = []\n",
    "# final_list = [data[(0+i)*100:(1+i)*100] for i in range(39)]\n",
    "# final_data = np.array(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jitu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch_size = 100\n",
      "ckpt_dir = ./models/mnist-vae/\n",
      "ckpt_epoch = 5\n",
      "learning_rate = 0.001\n",
      "n_hidden_gener_1 = 500\n",
      "n_hidden_gener_2 = 500\n",
      "n_hidden_recog_1 = 500\n",
      "n_hidden_recog_2 = 500\n",
      "n_input = 784\n",
      "n_z = 20\n",
      "num_samples = 60000\n",
      "sample_dir = ./samples/mnist-vae/\n",
      "summary_epoch = 1\n",
      "training_epochs = 100\n",
      "transfer_fct = <function softplus at 0x111b21ae8>\n",
      "\n",
      "enc/w1\n",
      "enc/b1\n",
      "enc/w2\n",
      "enc/b2\n",
      "enc/w3\n",
      "enc/b3\n",
      "enc/w4\n",
      "enc/b4\n",
      "gen/w1\n",
      "gen/b1\n",
      "gen/w2\n",
      "gen/b2\n",
      "gen/w3\n",
      "gen/b3\n",
      "beta1_power\n",
      "beta2_power\n",
      "enc/w1/Adam\n",
      "enc/w1/Adam_1\n",
      "enc/b1/Adam\n",
      "enc/b1/Adam_1\n",
      "enc/w2/Adam\n",
      "enc/w2/Adam_1\n",
      "enc/b2/Adam\n",
      "enc/b2/Adam_1\n",
      "enc/w3/Adam\n",
      "enc/w3/Adam_1\n",
      "enc/b3/Adam\n",
      "enc/b3/Adam_1\n",
      "enc/w4/Adam\n",
      "enc/w4/Adam_1\n",
      "enc/b4/Adam\n",
      "enc/b4/Adam_1\n",
      "gen/w1/Adam\n",
      "gen/w1/Adam_1\n",
      "gen/b1/Adam\n",
      "gen/b1/Adam_1\n",
      "gen/w2/Adam\n",
      "gen/w2/Adam_1\n",
      "gen/b2/Adam\n",
      "gen/b2/Adam_1\n",
      "gen/w3/Adam\n",
      "gen/w3/Adam_1\n",
      "gen/b3/Adam\n",
      "gen/b3/Adam_1\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/jitu/Documents/Fall/Business Data Science/finalproject/models/mnist-vae/mnist_vae_model-99\n",
      "Succesfully loaded model from /Users/jitu/Documents/Fall/Business Data Science/finalproject/models/mnist-vae/mnist_vae_model-99 at counter = 99\n",
      "3951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jitu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------after data--------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imsave\n",
    "\n",
    "\n",
    "def main(hparams):\n",
    "    # Set up some stuff according to hparams\n",
    "    set_up_dir(hparams.ckpt_dir)\n",
    "    set_up_dir(hparams.sample_dir)\n",
    "    print_hparams(hparams)\n",
    "\n",
    "    # encode\n",
    "    x_ph = tf.placeholder(tf.float32, [None, hparams.n_input], name='x_ph')\n",
    "    z_mean, z_log_sigma_sq = encoder(hparams, x_ph, 'enc', reuse=False)\n",
    "\n",
    "    # sample\n",
    "    eps = tf.random_normal((hparams.batch_size, hparams.n_z), 0, 1, dtype=tf.float32)\n",
    "    z_sigma = tf.sqrt(tf.exp(z_log_sigma_sq))\n",
    "    z = z_mean + z_sigma * eps\n",
    "\n",
    "    # reconstruct\n",
    "    logits, x_reconstr_mean = generator(hparams, z, 'gen', reuse=False)\n",
    "\n",
    "    # generator sampler\n",
    "    z_ph = tf.placeholder(tf.float32, [None, hparams.n_z], name='x_ph')\n",
    "    _, x_sample = generator(hparams, z_ph, 'gen', reuse=True)\n",
    "\n",
    "    # define loss and update op\n",
    "    total_loss = get_loss(x_ph, logits, z_mean, z_log_sigma_sq)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    update_op = opt.minimize(total_loss)\n",
    "\n",
    "    # Sanity checks\n",
    "    for var in tf.global_variables():\n",
    "        print (var.op.name)\n",
    "    print ('')\n",
    "\n",
    "    # Get a new session\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Model checkpointing setup\n",
    "    model_saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Attempt to restore variables from checkpoint\n",
    "    start_epoch = try_restore(hparams, sess, model_saver)\n",
    "\n",
    "    # Get data iterator\n",
    "    #iterator = mnist_data_iteratior()\n",
    "    #print('shape of x_ph', x_ph.shape)\n",
    "\n",
    "    \n",
    "    data = load_data('resized/')[:3900]\n",
    "    \n",
    "    final_data = []\n",
    "\n",
    "    for i in range(39):\n",
    "        final_data.append(data[(0+i)*100:(1+i)*100])\n",
    "\n",
    "    final_data = np.array(final_data)\n",
    "    print('---------after data--------')\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(start_epoch+1, hparams.training_epochs):\n",
    "        avg_loss = 0.0\n",
    "        num_batches = hparams.num_samples // hparams.batch_size\n",
    "        batch_num = 0\n",
    "        for x_batch_val in final_data:\n",
    "        #for (x_batch_val,_) in load_data('resized/'):\n",
    "            print('batchhhhhhh---------------',x_batch_val.shape)\n",
    "            batch_num += 1\n",
    "            #print('in for loop --------', x_batch_val)\n",
    "            \n",
    "#             print('-------------',type(x_batch_val))\n",
    "            x_batch_val = np.reshape(x_batch_val, (100, 28*28))\n",
    "#            print('size---------------',x_batch_val.shape)\n",
    "            \n",
    "            feed_dict = {x_ph: x_batch_val}\n",
    "            _, loss_val = sess.run([update_op, total_loss], feed_dict=feed_dict)\n",
    "            avg_loss += loss_val / hparams.num_samples * hparams.batch_size\n",
    "            print('average loss', avg_loss)\n",
    "            if batch_num % 100 == 0:\n",
    "                x_reconstr_mean_val = sess.run(x_reconstr_mean, feed_dict={x_ph: x_batch_val})\n",
    "\n",
    "                z_val = np.random.randn(hparams.batch_size, hparams.n_z)\n",
    "                x_sample_val = sess.run(x_sample, feed_dict={z_ph: z_val})\n",
    "                \n",
    "                print('------------before saving-------------')\n",
    "                print(x_reconstr_mean_val.shape)\n",
    "\n",
    "                save_images(np.reshape(x_reconstr_mean_val, [-1, 28, 28]),\n",
    "                                  [10,10],\n",
    "                                  '{}/reconstr_{:02d}_{:04d}.png'.format(hparams.sample_dir, epoch, batch_num))\n",
    "                print('------------after saving-------------')\n",
    "                save_images(np.reshape(x_batch_val, [-1, 28, 28]),\n",
    "                                  [10,10],\n",
    "                                  '{}/orig_{:02d}_{:04d}.png'.format(hparams.sample_dir, epoch, batch_num))\n",
    "                save_images(np.reshape(x_sample_val, [-1, 28, 28]),\n",
    "                                  [10,10],\n",
    "                                  '{}/sampled_{:02d}_{:04d}.png'.format(hparams.sample_dir, epoch, batch_num))\n",
    "\n",
    "\n",
    "        if epoch % hparams.summary_epoch == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch), 'Avg loss = {:.9f}'.format(avg_loss))\n",
    "\n",
    "        if epoch % hparams.ckpt_epoch == 0:\n",
    "            save_path = os.path.join(hparams.ckpt_dir, 'mnist_vae_model')\n",
    "            model_saver.save(sess, save_path, global_step=epoch)\n",
    "\n",
    "    save_path = os.path.join(hparams.ckpt_dir, 'mnist_vae_model')\n",
    "    model_saver.save(sess, save_path, global_step=hparams.training_epochs-1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    HPARAMS = Hparams()\n",
    "\n",
    "    HPARAMS.num_samples = 60000\n",
    "    HPARAMS.learning_rate = 0.001\n",
    "    HPARAMS.batch_size = 100\n",
    "    HPARAMS.training_epochs = 100\n",
    "    HPARAMS.summary_epoch = 1\n",
    "    HPARAMS.ckpt_epoch = 5\n",
    "\n",
    "    HPARAMS.ckpt_dir = './models/mnist-vae/'\n",
    "    HPARAMS.sample_dir = './samples/mnist-vae/'\n",
    "\n",
    "    main(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# from array import *\n",
    "# from random import shuffle\n",
    "\n",
    "# # Load from and save to\n",
    "# Names = [['./resized','train']]\n",
    "\n",
    "# for name in Names:\n",
    "\t\n",
    "# \tdata_image = array('B')\n",
    "# \tdata_label = array('B')\n",
    "\n",
    "# \tFileList = []\n",
    "# \tfor dirname in os.listdir(name[0])[1:]: # [1:] Excludes .DS_Store from Mac OS\n",
    "# \t\tpath = os.path.join(name[0],dirname)\n",
    "# \t\tfor filename in os.listdir(path):\n",
    "# \t\t\tif filename.endswith(\".png\"):\n",
    "# \t\t\t\tFileList.append(os.path.join(name[0],dirname,filename))\n",
    "\n",
    "# \tshuffle(FileList) # Usefull for further segmenting the validation set\n",
    "\n",
    "# \tfor filename in FileList:\n",
    "\n",
    "# \t\tlabel = int(filename.split('/')[2])\n",
    "\n",
    "# \t\tIm = Image.open(filename)\n",
    "\n",
    "# \t\tpixel = Im.load()\n",
    "\n",
    "# \t\twidth, height = Im.size\n",
    "\n",
    "# \t\tfor x in range(0,width):\n",
    "# \t\t\tfor y in range(0,height):\n",
    "# \t\t\t\tdata_image.append(pixel[y,x])\n",
    "\n",
    "# \t\tdata_label.append(label) # labels start (one unsigned byte each)\n",
    "\n",
    "# \thexval = \"{0:#0{1}x}\".format(len(FileList),6) # number of files in HEX\n",
    "\n",
    "# \t# header for label array\n",
    "\n",
    "# \theader = array('B')\n",
    "# \theader.extend([0,0,8,1,0,0])\n",
    "# \theader.append(int('0x'+hexval[2:][:2],16))\n",
    "# \theader.append(int('0x'+hexval[2:][2:],16))\n",
    "\t\n",
    "# \tdata_label = header + data_label\n",
    "\n",
    "# \t# additional header for images array\n",
    "\t\n",
    "# \tif max([width,height]) <= 256:\n",
    "# \t\theader.extend([0,0,0,width,0,0,0,height])\n",
    "# \telse:\n",
    "# \t\traise ValueError('Image exceeds maximum size: 256x256 pixels');\n",
    "\n",
    "# \theader[3] = 3 # Changing MSB for image data (0x00000803)\n",
    "\t\n",
    "# \tdata_image = header + data_image\n",
    "\n",
    "# \toutput_file = open(name[1]+'-images-idx3-ubyte', 'wb')\n",
    "# \tdata_image.tofile(output_file)\n",
    "# \toutput_file.close()\n",
    "\n",
    "# \toutput_file = open(name[1]+'-labels-idx1-ubyte', 'wb')\n",
    "# \tdata_label.tofile(output_file)\n",
    "# \toutput_file.close()\n",
    "\n",
    "# # gzip resulting files\n",
    "\n",
    "# for name in Names:\n",
    "# \tos.system('gzip '+name[1]+'-images-idx3-ubyte')\n",
    "# \tos.system('gzip '+name[1]+'-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
